{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a4fb02f",
   "metadata": {},
   "source": [
    "### First double multiplexer, then single phase tuner\n",
    "Including even ports template and odd ports template, use odd port nets for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ace8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "from jax import custom_vjp\n",
    "from jax import nn\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "CTYPE = jnp.complex64\n",
    "RTYPE = jnp.float32\n",
    "FIGDPI = 100\n",
    "FIGSIZE = (5,4)\n",
    "\n",
    "scale_factor = 128\n",
    "# double ring parameters\n",
    "r1 = 0.553\n",
    "r2 = 0.84\n",
    "dx = 0.\n",
    "# dx = 0.\n",
    "a = 0.98\n",
    "lr = 1e-3\n",
    "\n",
    "# initiate around the phase value\n",
    "around = np.pi/5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e65fad7",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ffbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block won't be changed.\n",
    "# this functions is a helper function to get the location; won't be modified\n",
    "def get_ABCD_index(num_ports, parity):\n",
    "    if parity==\"even\":\n",
    "        ind = jnp.arange(0, num_ports-1, 2)\n",
    "    elif parity==\"odd\":\n",
    "        ind = jnp.arange(1, num_ports-1, 2)\n",
    "    else:\n",
    "        raise ValueError(\"parity should be `even` or `odd`, but get:\", parity)\n",
    "    ind_A = (ind, ind)\n",
    "    ind_D = (ind+1, ind+1)\n",
    "    ind_B = (ind, ind+1)\n",
    "    ind_C = (ind+1, ind)\n",
    "    return ind_A, ind_B, ind_C, ind_D\n",
    "\n",
    "# these two functions can be isolated; won't be modified\n",
    "def double_transfer(theta, r1, r2, a, dx):\n",
    "    theta = -(theta + dx)\n",
    "    denom = 1+a**2*jnp.exp(2*1j*theta)*r1**2-2*a*jnp.exp(1j*theta)*r1*r2\n",
    "    numA = r1+a**2*jnp.exp(2*1j*theta)*r1-a*jnp.exp(1j*theta)*r2-a*jnp.exp(1j*theta)*r1**2*r2\n",
    "    numB = -1j*a*jnp.exp(1j*theta)*(-1+r1**2)*jnp.sqrt(1-r2**2)\n",
    "    numC = -1j*a*jnp.exp(1j*theta)*(-1+r1**2)*jnp.sqrt(1-r2**2)\n",
    "    numD = r1+a**2*jnp.exp(2*1j*theta)*r1-a*jnp.exp(1j*theta)*r2-a*jnp.exp(1j*theta)*r1**2*r2\n",
    "    return numA/denom, numB/denom, numC/denom, numD/denom\n",
    "\n",
    "def single_transfer(phi, r, a):\n",
    "    return jnp.exp(1j*jnp.pi+1j*phi)*(a-r*jnp.exp(-1j*phi))/(1.-r*a*jnp.exp(1j*phi))\n",
    "\n",
    "# activation can be separated as well\n",
    "def ring_activation(g, phi_b, alpha, a_act, r_act, xout):\n",
    "    # ring\n",
    "    dphi = 0.5*g*jnp.abs(xout)**2 + 0.5*phi_b\n",
    "    t_ring = jnp.exp(1j*(np.pi+dphi))*(a_act-r_act*jnp.exp(1j*dphi))/(1.0-r_act*a_act*jnp.exp(1j*dphi))\n",
    "    xout = (1j * jnp.sqrt(1.0-alpha) * t_ring) * xout\n",
    "    return xout\n",
    "\n",
    "@partial(jit, static_argnums=(0,))\n",
    "def get_transfer_even(num_ports, dA, dB, dC, dD):\n",
    "    \"\"\"\n",
    "    Get the even depths transfer matrix.\n",
    "    \"\"\"\n",
    "    ind_ABCD = get_ABCD_index(num_ports, \"even\")\n",
    "    T = jnp.eye(num_ports, dtype=CTYPE)\n",
    "    T = T.at[ind_ABCD[0]].set(dA)\n",
    "    T = T.at[ind_ABCD[1]].set(dB)\n",
    "    T = T.at[ind_ABCD[2]].set(dC)\n",
    "    T = T.at[ind_ABCD[3]].set(dD)\n",
    "    return T\n",
    "\n",
    "@partial(jit, static_argnums=(0,))\n",
    "def get_transfer_odd(num_ports, dA, dB, dC, dD):\n",
    "    \"\"\"\n",
    "    Get the odd depths transfer matrix.\n",
    "    \"\"\"\n",
    "    ind_ABCD = get_ABCD_index(num_ports, \"odd\")\n",
    "    T = jnp.eye(num_ports, dtype=CTYPE)\n",
    "    T = T.at[ind_ABCD[0]].set(dA)\n",
    "    T = T.at[ind_ABCD[1]].set(dB)\n",
    "    T = T.at[ind_ABCD[2]].set(dC)\n",
    "    T = T.at[ind_ABCD[3]].set(dD)\n",
    "    return T\n",
    "\n",
    "double_transfer_even_jit = jit(double_transfer)\n",
    "single_transfer_even_jit = jit(single_transfer)\n",
    "double_transfer_odd_jit = jit(double_transfer)\n",
    "single_transfer_odd_jit = jit(single_transfer)\n",
    "ring_activation_jit = jit(ring_activation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfb1a7e0",
   "metadata": {},
   "source": [
    "### Define network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f435adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single ring parameters\n",
    "rsingle = r1\n",
    "asingle = a\n",
    "\n",
    "# activation function parameters\n",
    "# g = np.pi\n",
    "# phi_b = 0\n",
    "# alpha = 0.1\n",
    "# a_act = 0.9\n",
    "# r_act = 0.9\n",
    "\n",
    "g = 0.314\n",
    "phi_b = -0.1\n",
    "alpha = 0.1\n",
    "a_act = 0.9\n",
    "r_act = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbacdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = ports\n",
    "num_ports = 16\n",
    "num_depth = num_ports\n",
    "num_layers = 2\n",
    "\n",
    "# for even and odd depths (assume the first depth is even.)\n",
    "num_depth_even = int(jnp.ceil(num_depth/2))\n",
    "num_depth_odd = int(jnp.floor(num_depth/2))\n",
    "num_ports_even = int(jnp.floor(num_ports/2))\n",
    "num_ports_odd = int(jnp.ceil(num_ports/2) - 1)\n",
    "num_total_even = num_layers*num_depth_even*num_ports_even \n",
    "num_total_odd = num_layers*num_depth_odd*num_ports_odd\n",
    "\n",
    "# get the optimization parameters\n",
    "# once the structure is setup, the shape of transfer matrix is setup.\n",
    "key, subkey1, subkey2, subkey3, subkey4 = random.split(key, 5)\n",
    "\n",
    "# our parameters\n",
    "theta_even = random.uniform(subkey1, \n",
    "                            shape=(num_layers, num_depth_even, \n",
    "                                   num_ports_even),\n",
    "                           )*around\n",
    "theta_odd = random.uniform(subkey2, \n",
    "                           shape=(num_layers, num_depth_odd, \n",
    "                                  num_ports_odd),\n",
    "                          )*around\n",
    "\n",
    "phi_even = random.uniform(subkey3, \n",
    "                          shape=(num_layers, num_depth_even, \n",
    "                                 num_ports_even),\n",
    "                         )*around\n",
    "phi_odd = random.uniform(subkey4, \n",
    "                         shape=(num_layers, num_depth_odd, \n",
    "                                num_ports_odd),\n",
    "                        )*around\n",
    "\n",
    "params = {}\n",
    "params['theta_even'] = theta_even\n",
    "params['theta_odd'] = theta_odd\n",
    "params['phi_even'] = phi_even\n",
    "params['phi_odd'] = phi_odd\n",
    "init_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0af520",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_matmul = jit(jnp.matmul)\n",
    "\n",
    "# let's focus on the double ring only first\n",
    "@jit\n",
    "def clements_evenports(r1, r2, a, dx, rsingle, asingle, \n",
    "                       theta_even, theta_odd, phi_even, phi_odd, xin):\n",
    "    \"\"\"\n",
    "    1-layer clements setup.\n",
    "    theta_even: (num_depth, num_even_devices)\n",
    "    theta_odd: (num_depth, num_odd_devices)\n",
    "    phi_even: (num_depth, num_even_devices)\n",
    "    phi_odd: (num_depth, num_even_devices)\n",
    "    xin: (x_vec_shape, batch_size)\n",
    "    \"\"\"\n",
    "    dAeven, dBeven, dCeven, dDeven = double_transfer_even_jit(theta_even, r1, r2, a, dx)\n",
    "    dAodd, dBodd, dCodd, dDodd = double_transfer_odd_jit(theta_odd, r1, r2, a, dx)\n",
    "    phase_even = single_transfer_even_jit(phi_even, rsingle, asingle)\n",
    "    phase_odd = single_transfer_odd_jit(phi_odd, rsingle, asingle)\n",
    "    # if the ports number is even\n",
    "    for d in range(int(num_depth/2)):\n",
    "        T_even = get_transfer_even(num_ports, \n",
    "                                   dAeven[d]*phase_even[d], dBeven[d]*phase_even[d], \n",
    "                                   dCeven[d], dDeven[d])\n",
    "        T_odd = get_transfer_odd(num_ports, \n",
    "                                 dAodd[d]*phase_odd[d], dBodd[d]*phase_odd[d], \n",
    "                                 dCodd[d], dDodd[d])\n",
    "        xin = jit_matmul(T_odd, jit_matmul(T_even, xin))\n",
    "    return xin\n",
    "\n",
    "@jit\n",
    "def clements_oddports(r1, r2, a, dx, rsingle, asingle, \n",
    "                      theta_even, theta_odd, phi_even, phi_odd, xin):\n",
    "    \"\"\"\n",
    "    1-layer clements setup.\n",
    "    theta_even: (num_depth, num_even_devices)\n",
    "    theta_odd: (num_depth, num_odd_devices)\n",
    "    phi_even: (num_depth, num_even_devices)\n",
    "    phi_odd: (num_depth, num_even_devices)\n",
    "    xin: (x_vec_shape, batch_size)\n",
    "    \"\"\"\n",
    "    dAeven, dBeven, dCeven, dDeven = double_transfer_even_jit(theta_even, r1, r2, a, dx)\n",
    "    dAodd, dBodd, dCodd, dDodd = double_transfer_odd_jit(theta_odd, r1, r2, a, dx)\n",
    "    phase_even = single_transfer_even_jit(phi_even, rsingle, asingle)\n",
    "    phase_odd = single_transfer_odd_jit(phi_odd, rsingle, asingle)\n",
    "    \n",
    "    # if the ports number is odd.\n",
    "    for d in range(int(num_depth/2)):\n",
    "        T_even = get_transfer_even(num_ports, \n",
    "                                   dAeven[d]*phase_even[d], dBeven[d]*phase_even[d], \n",
    "                                   dCeven[d], dDeven[d])\n",
    "        T_odd = get_transfer_odd(num_ports, \n",
    "                                 dAodd[d]*phase_odd[d], dBodd[d]*phase_odd[d], \n",
    "                                 dCodd[d], dDodd[d])\n",
    "        xin = jit_matmul(T_odd, jit_matmul(T_even, xin))\n",
    "        \n",
    "    # note that jax won't jump error if we exeed the array sizes.\n",
    "    T_even = get_transfer_even(num_ports, \n",
    "                               dAeven[d+1]*phase_even[d+1], dBeven[d+1]*phase_even[d+1], \n",
    "                               dCeven[d+1], dDeven[d+1])\n",
    "    xin = jit_matmul(T_even, xin)\n",
    "    return xin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c8b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def network_even(theta_even, theta_odd, phi_even, phi_odd, xin):\n",
    "    xin = clements_evenports(r1, r2, a, dx, rsingle, asingle, \n",
    "                              theta_even[0], theta_odd[0], \n",
    "                              phi_even[0], phi_odd[0], xin)\n",
    "    xin = ring_activation(g, phi_b, alpha, a_act, r_act, xin)\n",
    "    for l in range(1, num_layers):\n",
    "        xin = clements_evenports(r1, r2, a, dx, rsingle, asingle, \n",
    "                                  theta_even[l], theta_odd[l], \n",
    "                                  phi_even[l], phi_odd[l], xin)\n",
    "        xin = ring_activation(g, phi_b, alpha, a_act, r_act, xin)\n",
    "    \n",
    "    xin = jnp.abs(xin)**2\n",
    "    return xin\n",
    "\n",
    "@jit\n",
    "def network_odd(theta_even, theta_odd, phi_even, phi_odd, xin):\n",
    "    xin = clements_oddports(r1, r2, a, dx, rsingle, asingle, \n",
    "                             theta_even[0], theta_odd[0], \n",
    "                             phi_even[0], phi_odd[0], xin)\n",
    "    xin = ring_activation(g, phi_b, alpha, a_act, r_act, xin)\n",
    "    for l in range(1, num_layers):\n",
    "        xin = clements_oddports(r1, r2, a, dx, rsingle, asingle, \n",
    "                                 theta_even[l], theta_odd[l], \n",
    "                                 phi_even[l], phi_odd[l], xin)\n",
    "        xin = ring_activation(g, phi_b, alpha, a_act, r_act, xin)\n",
    "    \n",
    "    xin = jnp.abs(xin)**2\n",
    "    return xin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f775d0e",
   "metadata": {},
   "source": [
    "### MNIST dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a962c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_data import *\n",
    "\n",
    "mnist_dp = MNISTDataProcessor()\n",
    "data_N16 = mnist_dp.fourier(2)\n",
    "x_train, y_train, x_test, y_test = data_N16\n",
    "x_train = jnp.array(x_train.T, dtype=CTYPE)*scale_factor\n",
    "y_train = jnp.array(y_train.T, dtype=RTYPE)\n",
    "x_test = jnp.array(x_test.T, dtype=CTYPE)*scale_factor\n",
    "y_test = jnp.array(y_test.T, dtype=RTYPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b23ee29",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss + Optimizer + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1d4cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "from jax.experimental import optimizers\n",
    "import time\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88713c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(params, xin, Y):\n",
    "    xin = network_even(params['theta_even'], params['theta_odd'], \n",
    "                       params['phi_even'], params['phi_odd'], xin)\n",
    "    y_pred = xin[:10, :]\n",
    "    log_softmax_y_pred = nn.log_softmax(y_pred, axis=0)\n",
    "    l = - log_softmax_y_pred*Y[:10, :]\n",
    "    return l.sum(axis=0).mean()\n",
    "\n",
    "def accuracy(params, x, y, batch_size):\n",
    "    num_batches = int(x.shape[1]/batch_size)\n",
    "    y_pred = jnp.empty((10,0), dtype=RTYPE)\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        x_sub = x[:, batch*batch_size:(batch+1)*batch_size]\n",
    "        x_pred = network_even(params['theta_even'], params['theta_odd'], \n",
    "                              params['phi_even'], params['phi_odd'], x_sub)\n",
    "        y_pred = jnp.hstack((y_pred, x_pred[:10,:]))\n",
    "    \n",
    "    y_pred = jnp.argmax(y_pred, axis=0)\n",
    "    y_true = y[:, :y_pred.shape[0]]\n",
    "    y_true = jnp.argmax(y_true, axis=0)\n",
    "    acc = jnp.sum(jnp.equal(y_pred, y_true))/float(y_pred.shape[0])\n",
    "    return acc, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf49dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xor(x_train, y_train, x_test, y_test, params, num_epochs, batch_size=32, lr=1e-3, optimizer=optimizers.adam):\n",
    "    jit_value_grad  = jit(value_and_grad(loss, (0, )))\n",
    "    _, num_train_samples = x_train.shape\n",
    "    num_train_batches = int(num_train_samples/batch_size)\n",
    "    \n",
    "    @jit\n",
    "    def update(params, x, y, opt_state):\n",
    "        # we record the loss parameters before the updated one\n",
    "        value, grads = jit_value_grad(params, x, y)\n",
    "        opt_state = opt_update(0, grads[0], opt_state)\n",
    "        return get_params(opt_state), opt_state, value\n",
    "\n",
    "    # Defining an optimizer in Jax\n",
    "    opt_init, opt_update, get_params = optimizer(lr)\n",
    "    opt_state = opt_init(params)\n",
    "    \n",
    "    # record by iteration steps (record loss for every batch)\n",
    "    loss_list = []\n",
    "    # record by epoch\n",
    "    acc_train_list = []\n",
    "    acc_test_list = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    acc_test, _, _ = accuracy(params, x_test, y_test, batch_size)\n",
    "    acc_train, _, _ = accuracy(params, x_train, y_train, batch_size)\n",
    "    acc_train_list.append(acc_train)\n",
    "    acc_test_list.append(acc_test)\n",
    "    \n",
    "    t = trange(num_epochs, desc='MNIST', position=0, leave=True)\n",
    "    for epoch in t:\n",
    "        for batch in range(num_train_batches):\n",
    "            params, opt_state, l = update(params, \n",
    "                                          x_train[:, batch*batch_size:(batch+1)*batch_size], \n",
    "                                          y_train[:, batch*batch_size:(batch+1)*batch_size], \n",
    "                                          opt_state)\n",
    "            loss_list.append(l)\n",
    "\n",
    "        # evaluation process\n",
    "        acc_test, _, _ = accuracy(params, x_test, y_test, batch_size)\n",
    "        acc_train, _, _ = accuracy(params, x_train, y_train, batch_size)\n",
    "        acc_train_list.append(acc_train)\n",
    "        acc_test_list.append(acc_test)\n",
    "#         t.set_description('Train accuracy=%g, ' % acc_train + 'Test accuracy=%g, ' % acc_test + 'last batch loss=%g'% l)\n",
    "        t.set_description('Test accuracy=%g' % acc_test)\n",
    "#         print(\"epoch: \", epoch)\n",
    "#         print(\"time elapse: \", time.time()-start_time)\n",
    "#         print(\"train accuracy: \", acc_train)\n",
    "#         print(\"test accuracy: \", acc_test)\n",
    "    \n",
    "    return loss_list, acc_train_list, acc_test_list, params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebc32287",
   "metadata": {},
   "source": [
    "### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbaaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "# num_epochs = 200\n",
    "# lr = 2e-5\n",
    "# batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ca23e56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MNIST:   0%|          | 0/200 [00:00<?, ?it/s]2023-06-13 10:57:49.355860: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:55] \n",
      "********************************\n",
      "Slow compile?  XLA was built without compiler optimizations, which can be slow.  Try rebuilding with -c opt.\n",
      "Compiling module jit_update.27288\n",
      "********************************\n",
      "Test accuracy=0.878906:  49%|████▉     | 98/200 [09:04<08:11,  4.82s/it] "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "loss_list, acc_train_list, acc_test_list, trained_params = run_xor(x_train, y_train, \n",
    "                                                                   x_test, y_test, \n",
    "                                                                   params, num_epochs, \n",
    "                                                                   batch_size=batch_size,\n",
    "                                                                   lr=lr)\n",
    "print(\"Total training time: \", time.time()-start_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8524954b",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e28201",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, y_pred, y_true = accuracy(trained_params, x_test, y_test, batch_size)\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "conf_m = confusion_matrix(y_true, y_pred)\n",
    "conf_m = conf_m / conf_m.astype(np.float).sum(axis=1)\n",
    "df_cm = pd.DataFrame(conf_m)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE, dpi=FIGDPI)\n",
    "sn.heatmap(np.round(df_cm*100, 1), annot=True, annot_kws={\"size\": 10})\n",
    "# plt.savefig(filepath+prename+posname+'_confusion.svg', dpi=FIGDPI, format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1153e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=FIGSIZE, dpi=FIGDPI) \n",
    "ax.plot(acc_test_list, label='test')\n",
    "ax.plot(acc_train_list, label='train')\n",
    "ax.legend()\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_ylim([0.0,1])\n",
    "# plt.savefig(filepath+prename+posname+'_accuracy.svg', dpi=FIGDPI, format=\"svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
